\documentclass[11pt]{article}
\usepackage{listings,fancyhdr,hyperref,graphicx,subfig,appendix}
\usepackage{fullpage}
\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}
\fancyfoot[R]{Page \thepage}
\fancyfoot[L]{Data sources for MGT285}
\renewcommand{\footrulewidth}{0.4 pt}
\renewcommand{\headrulewidth}{0 pt}
\hypersetup{colorlinks = true, linkcolor = blue, citecolor = blue}
\begin{document}

\begin{flushleft}
	\vspace*{0.3in}
	\Huge \textbf{Data sources for MGT285}\\
	\Large \today \\
	\vspace{0.25in}
	\textsc{Hugh Crockford}
	\vspace{0.25in}
	\hrule
	\hrule
	\tableofcontents
\end{flushleft}

	\vspace{0.75in}
	%	\renewcommand{\abstractname}{Executive Summary}
	% \end{abstract}

\newpage
\section{Introduction}
	There is a large amount of data available from a variety of credible and not so credible sources online.
	Historical time series of macroeconomic indicators and indexes can be useful variables to consider including in any long duration time series model, and the rise of the 'social web' has produced reams of data to aid and inform decision making.


	This vignette describes a variety of data sources that may be useful during any modelling project.

\section{Sources of Historical Time Series data}
	Variables such as population, income, and CPI are economy wide metrics that can explain some of the variability present in a dataset.
	Historical pricing information for commodities and financial instruments are also potentially important when developing costing/pricing or demand models.
	Many exchange sites will offer these data for a hefty subscription fee, however most can be found online for free

	\subsection{Financial Time Series - Index and pricing data}
	Some data can be found with a simple Google search, however many sites are of questionable quality or will request payment for full access.


	Ycharts \footnote{http://ycharts.com/} appears to be the best bang-for-buck at \$49 month. 
	This site has many data series, allows custom charts and data download, and also includes an Excel add-in with purchase of professional membership.
	Approximate data can also be extracted from freely available graphs as described in \hyperref[graph]{Appendix C}


	A Bloomberg terminal subscription provides access to a plethora of Industry grade datasets, including real-time market data and historical quotes.
	A subscription to Bloomberg is a minimum \$2000 pcm however many financial business' have a subscription that can be used to download series as needed.
	The UC Library system unfortunately does not have access to Bloomberg data.
	Bloomberg also offers an \hyperref[api]{API} \footnote{http://www.openbloomberg.com/open-api/} that allows data to be dynamically loaded into excel.

	Quandl \footnote{http://www.quandl.com/} appears to be the most useful online data source, with large amounts of quality data freely accessible.
	It aggregates over 5 million financial, economic, and social datasets from around the web with primary reference information.
	All datasets are manually accessible and downloadable from the search engine on their homepage, and can be accessed from within R using the package 'Quandl', and via an Excel plugin. 

	% \newpage
	
	\subsection{Demographic information}

	Population and demographic information can be found around the web, however the most trusted source is American Census Bureau \footnote{http://www.census.gov/}.
	Their website is a little difficult to navigate, and the 'American fact-finder'\footnote{http://factfinder2.census.gov/faces/nav/jsf/pages/index.xhtml} is the search engine to locate area specific data.
	The most powerful way to access the census data is through their \hyperref[api]{API}, with which quereies can be built and run over multiple areas and variables.


	\section{Social Media Data}
	The explosion in social media has sparked interest in dynamically monitoring the massive amounts of user generated data to generate insights into product choices, pending pandemics, and even stock prices.

	\subsection{Google Trends/Search volumes}
	Most consumers employ a search engine for everything from what car to buy to finding out if they have the flu or tuberculosis.
	Tracking search volume across time can provide insights into what the 'crowd' is researching, from which inferences can be made.
	The first application of this was when Google succesfully predicted a Flu epidemic based on seach volumes before the mighty CDC\cite{Ginsberg2009} .
	Google trends \footnote{http://www.google.com/trends/explore\#cmpt=q} is a demonstration of this capability. 
	Keywords can be compared (e.g. Toyota to Ford) and relative search volumes found, and news items associated with peaks. 
	The data can be downloaded into a csv (comma seperated value) that is easily loaded into excel, and queries can be based on location, timeframe and category.
	The google trends API can also be accessed by R packages 'rGtrend' and 'RGoogleTrends'
		

	Related to google trends is Wikipedia page views, which has shown to be indicitive of future stock price moves \cite{Moat2013}.
	Wikipedia page statistics can be accessed manually \footnote{http://stats.grok.se/}, or programatically via a RESTful \hyperref[api]{API}, developed in a paper from Microsoft Research \cite{Peetz}. 

	\subsection{Twitter/Facebook}
	Besides photos of what people had for breakfast, twitter data can be analysed to reveal people's brand awareness and perceptions, and can reveal future market events \cite{Ruiz2012}.
	Many people are actively watching social media and trading on this data, with twitter sentiment analysis shown to beat various metrics in predicting of DJIA closing values \cite{Bollen2011}.
	The importance traders place on information from social media was demonstrated in May 2013 when the Asociated press' twitter account was hacked and a false tweet reproting the White House had been bombed was released. 
	The false tweet erased \$200 billion from the US stock market, withe the Dow falling almost 150  points in 2 minutes, and oil and tbond futures following \cite{wsj13}. 

	Historical tweets (last 7 days) can be accessed from twitter's \hyperref[api]{API} \footnote{https://dev.twitter.com/docs/api} and interpreted using Natural Language Processing tools.
	There are also various services that present trending tweets geographically \footnote{https://dev.twitter.com/docs/api}  and by hashtag \footnote{http://www.hashtags.org/}


\newpage

\appendix
\appendixpage
\addappheadtotoc
	
\section{R Resources} \label{app:mod}
	R is an open source statistical programming language widely used in academia and business.
	It has been suggested to replace shitty old excel, which has been impolicated in some high profile errors (london whale, mf global, lehman) attributed to the use of excel\cite{excel13}.
	While the initial learngin curve cna be steep, the scripting and scaling capabilites mean an itial time investment to learn will pay dividends int he future.
	In addition, many of the techniques discussed in this vignette are implemented directly in R, allowing seamless integration of data collection and modelling.


	There are many books available to assist learning R, and a wealth of online information. 
	For those not used to the command line interface, R Studio (http://www.rstudio.com/) is a free GUI (graphical user interface) which has many tools to assist learning R.

	There are nbumerous classes on main campus to learn R, however most of the learning happens by doing.
	More advanced classes that recuire a solid foundation in R include STA 141 and STA 242


	\section{Appplication Programming Interface (API)}\label{api}
	Many data rich websites recognise navigating menu's and downloading individual files is laborious so have developed API's (Application programming interface) to allow scripted access to data.
	For any project recquireing many queries of a database, it is worth scripting the requests in an HTTP GET or POST to allow many interations to be run to download recwuired data.

	Some api's (e.g. quandl,twitter) will require a registration key so that usage can be tracked.


	The predominant web API is REST (Representational state transfer), which processes requests in the form of HTTP GET or POST querey's.
	Each REST server will have its own methods and documentation will be provided to allow scripted querey development.
	Examples of API's include Twitter recent tweets, US Census, NOAA, and many more.


	\section{Extracting data from a graph}\label{graph}
	Often a graph can be found with recquired time series, however the data used to geneerate the figure are unavailable.
	An approximation of underlying data can be generated by reading the position of lines/points versus calibrated known points. 
	Numerous tools exist to complete this task, the most uder friendly I have come across being WebPlotDigitizer, an online tool (http://arohatgi.info/WebPlotDigitizer/). 
	This can also be completed within R using the 'digitize' package.

\newpage
\section{Bibliography}
\bibliographystyle{unsrt}
\bibliography{refs}

\end{document}

